{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6A-bYtNNnd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KSUpX1-P1uv"
      },
      "source": [
        "# Part 4: Fine-tuning pretrained language models\n",
        "\n",
        "Acknowledgement: This notebook is based on the code from https://mccormickml.com/2019/07/22/BERT-fine-tuning/. Credit to \n",
        "Chris McCormick and Nick Ryan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l47jS3ifQJz3"
      },
      "source": [
        "## Background\n",
        "\n",
        "Fine-tuning BERT on our task of interest takes some setup. Although these steps are done for you, please take a moment to look through them and make sure you understand their purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00GDvCJucnpv"
      },
      "source": [
        "Install the [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) package that contains the pretrained BERT models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AIe26NRcuYZ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p18ZuvMUcyQE"
      },
      "source": [
        "Set the random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkGc79g2c3DL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJynoDMfc6Tr"
      },
      "source": [
        "Run the following cells to download the verbal arithmetic dataset from the CSC413 webpage and load it into a `DataFrame`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFvJibK2wEdP"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN77phG-f3DY"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading verbal arithmetic dataset')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://csc413-uoft.github.io/2021/assets/misc/'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./PA03_data_20_train.csv'):\n",
        "  wget.download(url + 'PA03_data_20_train.csv', './PA03_data_20_train.csv')\n",
        "  print('Done downloading training data')\n",
        "else:\n",
        "  print('Already downloaded training data')\n",
        "\n",
        "if not os.path.exists('./PA03_data_20_test.csv'):\n",
        "  wget.download(url + 'PA03_data_20_test.csv', './PA03_data_20_test.csv')\n",
        "  print('Done downloading test data')\n",
        "else:\n",
        "  print('Already downloaded test data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovSDN8kBf_zF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./PA03_data_20_train.csv\", header=0, names=[\"index\", \"input\", \"label\"])\n",
        "\n",
        "print(\"Number of data points: \", df.shape[0])\n",
        "sampled = df.sample(10)\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyNFphizgCiM"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. For this we can use the `AutoTokenizer` from the `transformers` library.\n",
        "\n",
        "As mentioned in the assignment handout, we will use [MathBERT](https://arxiv.org/abs/2106.07340), which uses the same architecture as BERT, but has been pretrained on text from pre-kindergarten, high-school, and college graduate level mathematical content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JazZH2kzgHF_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('tbs17/MathBERT', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwJi4x1sgYKX"
      },
      "outputs": [],
      "source": [
        "inputs = df.input.values\n",
        "labels = df.label.values\n",
        "print(\"Train data size \", len(inputs))\n",
        "print('* Original:  ', inputs[0])\n",
        "# Print the sentence split into tokens.\n",
        "print('* Tokenized: ', bert_tokenizer.tokenize(inputs[0]))\n",
        "# Print the sentence mapped to token ids.\n",
        "print('* Token IDs: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(inputs[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RenfgCJg4Fi"
      },
      "source": [
        "### Formatting the inputs\n",
        "\n",
        "In order to use BERT for fine-tuning, we need to format the inputs in a way that matches the inputs of the pretraining step. In short, we need to:\n",
        "\n",
        "1. Add special tokens to the start and end of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWOVuJohI3q"
      },
      "source": [
        "#### Special Tokens\n",
        "\n",
        "**`[SEP]`**\n",
        "\n",
        "At the end of every sentence, we need to append the special `[SEP]` token. \n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n",
        "\n",
        "**`[CLS]`**\n",
        "\n",
        "For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n",
        "\n",
        "On the output of the final transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier*.\n",
        "\n",
        ">  \"The first token of every sequence is always a special classification token (`[CLS]`). The final hidden state\n",
        "corresponding to this token is used as the aggregate sequence representation for classification\n",
        "tasks.\" (from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Also, because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrvaiOR2hAEH"
      },
      "source": [
        "#### Sentence Length & Attention Mask\n",
        "\n",
        "The sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n",
        "\n",
        "BERT has two constraints:\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. \n",
        "\n",
        "The \"Attention Mask\" is simply an array of 0s and 1s indicating which tokens are padding and which aren't.\n",
        "\n",
        "In our dataset, all sentences have three word tokens. However, we set the max length of sentence to 7 in this example to show what paddings will be in real world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irjbFrNapXjn"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length.\n",
        "MAX_LEN = 7\n",
        "\n",
        "# Print BERTs special PAD token and its index in the vocabulary\n",
        "print(f'Padding token: \"{bert_tokenizer.pad_token}\", ID: {bert_tokenizer.pad_token_id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-jDt4i3hFea"
      },
      "source": [
        "Luckily, the `BertTokenizer` object from the transformers library makes it easy to preprocess our input text correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3JJmMOdqeoO"
      },
      "outputs": [],
      "source": [
        "tokenized_inputs = bert_tokenizer(\n",
        "    inputs.tolist(),          # Input text\n",
        "    add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "    padding='max_length',     # pad to a length specified by the max_length\n",
        "    max_length=MAX_LEN,       # truncate all sentences longer than max_length\n",
        "    return_tensors='pt',      # return everything we need as PyTorch tensors\n",
        ")\n",
        "\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_masks = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', tokenized_inputs['input_ids'][0])\n",
        "print('* Token IDs:', tokenized_inputs['attention_mask'][0])\n",
        "print('* Tokenized:', bert_tokenizer.decode(tokenized_inputs['input_ids'][0]))\n",
        "print('* Attention_mask', tokenized_inputs['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCMCg2B-xHZU"
      },
      "source": [
        "### Training & Validation Split\n",
        "\n",
        "Let's divide up our data into a train set (70%) and a validation set (30%).\n",
        "\n",
        "We'also create an iterator for our dataset using the torch `DataLoader` class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4WUEsPnxege"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "def train_valid_split(input_ids, attention_masks, labels, batch_size=32):\n",
        "    # Use 70% for training and 30% for validation.\n",
        "    train_inputs, validation_inputs,  train_masks, validation_masks, train_labels, validation_labels = train_test_split(\n",
        "        input_ids, attention_masks, labels, random_state=SEED, test_size=0.3, stratify=labels\n",
        "    )\n",
        "\n",
        "    print('example train_input:    ', train_inputs[0])\n",
        "    print('example attention_mask: ', train_masks[0])\n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "    # Create the DataLoader for our training set.\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    # Create the DataLoader for our validation set.\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_dataloader = DataLoader(validation_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, validation_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW8M1GiAx14E"
      },
      "outputs": [],
      "source": [
        "bert_train_dataloader, bert_validation_dataloader = train_valid_split(\n",
        "    input_ids=input_ids,\n",
        "    attention_masks=attention_masks,\n",
        "    labels=labels,\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm_NPA-o0m3v"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkp-UJdO0xbF"
      },
      "source": [
        "### Question 1: Add a classifier to BERT [1pts]\n",
        "\n",
        "Here, we will add a simple classifier to the [BertModel](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/bert#transformers.BertModel) provided by the Transformers library.\n",
        "\n",
        "Your tasks are:\n",
        "\n",
        "1. In `__init__`, add a linear classifier that will map BERTs `[CLS]` token representation to the unnormalized output probabilities for each class (`logits`).\n",
        "2. In `forward`, pass BERTs `[CLS]` token representation to this new classifier to produce the `logits`.\n",
        "\n",
        "In total, you won't have to write more than three new lines of code. See the comments in the code for help!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbG9zbiz05OT"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class BertForSentenceClassification(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        ##### START YOUR CODE HERE #####\n",
        "        # Add a linear classifier that map BERTs [CLS] token representation to the unnormalized\n",
        "        # output probabilities for each class (logits).\n",
        "        # Notes: \n",
        "        #  * See the documentation for torch.nn.Linear\n",
        "        #  * You do not need to add a softmax, as this is included in the loss function\n",
        "        #  * The size of BERTs token representation can be accessed at config.hidden_size\n",
        "        #  * The number of output classes can be accessed at config.num_labels\n",
        "        self.classifier = ...\n",
        "        ##### END YOUR CODE HERE #####\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, labels=None, **kwargs):\n",
        "        outputs = super().forward(**kwargs)\n",
        "        ##### START YOUR CODE HERE #####\n",
        "        # Pass BERTs [CLS] token representation to this new classifier to produce the logits.\n",
        "        # Notes:\n",
        "        #  * The [CLS] token representation can be accessed at outputs.pooler_output\n",
        "        cls_token_repr = ...\n",
        "        logits = ... \n",
        "        ##### END YOUR CODE HERE #####\n",
        "        if labels is not None:\n",
        "            outputs = (logits, self.loss(logits, labels))\n",
        "        else:\n",
        "            outputs = (logits,)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmeBvaXY2Mcm"
      },
      "source": [
        "### Question 2: Fine-tune BERT [0pts]\n",
        "\n",
        "In this section, we will instantiate our pretrained BERT model + the new classifier, and train both on our verbal arithmetic dataset for a few epochs.\n",
        "\n",
        "As mentioned in the assignment handout, we will use [MathBERT](https://arxiv.org/abs/2106.07340), which uses the same architecture as BERT, but has been pretrained on text from pre-kindergarten, high-school, and college graduate level mathematical content.\n",
        "\n",
        "\n",
        "> Although the code is written for you, please read it first to understand what it is doing. Additionally, running this code and making sure the model can be fine-tuned helps you check your implementation from Question 1. __Note__: This may print a warning: _\"Some weights of the model checkpoint at...\"_ which you can ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeM-sMbq3Tem"
      },
      "outputs": [],
      "source": [
        "mathbert = BertForSentenceClassification.from_pretrained(\n",
        "    \"tbs17/MathBERT\",  # the name of the pretrained model\n",
        "    num_labels=3,      # the number of classes in our downstream task \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tbxRsT3zQ5A"
      },
      "source": [
        "The following cell prints information about the models parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEuhZX6f3XiI"
      },
      "outputs": [],
      "source": [
        "# Model parameters visualization\n",
        "params = list(mathbert.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer Layer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-io5NMws3ckc"
      },
      "source": [
        "The next cell defines fairly standard train and evaluation loops in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_gVnpy13iqC"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "import time\n",
        "import datetime\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round(elapsed, 2))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def get_optimizer_and_scheduler(model, total_steps, lr=2e-5, weight_decay=0.01):\n",
        "    # Apply weight decay to all parameters beside the biases or LayerNorm weights\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': weight_decay},\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "        }\n",
        "    ]\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        # Warmup learning rate for first 10% of training steps\n",
        "        num_warmup_steps=int(0.10 * total_steps), \n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def train_model(model, epochs, train_dataloader, validation_dataloader):\n",
        "    # Use GPU, if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup optimizer and LR scheduler \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    optimizer, scheduler = get_optimizer_and_scheduler(\n",
        "        model, total_steps, lr=5e-5, weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    loss_values = []\n",
        "    eval_accs = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        t0 = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(train_dataloader, unit=\"batch\") as train_pbar:\n",
        "            for batch in train_pbar:\n",
        "                train_pbar.set_description(f\"Training (epoch {epoch + 1})\")\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_input_mask = batch[1].to(device)\n",
        "                b_labels = batch[2].to(device)\n",
        "\n",
        "                model.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # This will return the loss because we have provided the `labels`.\n",
        "                outputs = model(\n",
        "                    input_ids=b_input_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels\n",
        "                )\n",
        "                \n",
        "                # The call to `model` always returns a tuple, so we need to pull the \n",
        "                # loss value out of the tuple.\n",
        "                _, loss = outputs\n",
        "\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"  * Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  * Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "            \n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            \n",
        "            with torch.no_grad():        \n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have\n",
        "                # not provided labels.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                outputs = model(\n",
        "                    input_ids=b_input_ids, \n",
        "                    attention_mask=b_input_mask\n",
        "                )\n",
        "            \n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            logits = outputs[0]\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        avg_eval_acc = eval_accuracy/nb_eval_steps\n",
        "        print(\"  * Accuracy: {0:.2f}\".format(avg_eval_acc))\n",
        "        print(\"  * Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "        eval_accs.append(avg_eval_acc)\n",
        "    print(\"Training complete in {}\".format(format_time(time.time() - start)))\n",
        "    return loss_values, eval_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51t6bKJR2SbP"
      },
      "source": [
        "Finally, run the following cell to fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k7F9xJD3pP4"
      },
      "outputs": [],
      "source": [
        "# About 2-3 seconds per epoch using GPU\n",
        "mathbert_loss_vals, mathbert_eval_accs = train_model(\n",
        "    model=mathbert,\n",
        "    epochs=3,\n",
        "    train_dataloader=bert_train_dataloader,\n",
        "    validation_dataloader=bert_validation_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e61V4DVD34Nr"
      },
      "source": [
        "Once the model is trained, we can plot some performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3J-c8wQ5gDj"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_loss_and_acc(loss_vals, eval_accs):\n",
        "    sns.set(style='darkgrid')\n",
        "    sns.set(font_scale=1.5)\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "    fig, ax1 = plt.subplots(1,1)\n",
        "    ax1.plot(loss_vals, 'b-o', label = 'training loss')\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(eval_accs, 'y-o', label = 'validation accuracy')\n",
        "    ax2.set_title(\"Training loss and validation accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\", color='b')\n",
        "    ax2.set_ylabel(\"Accuracy\", color='y')\n",
        "    ax1.tick_params(axis='y', rotation=0, labelcolor='b' )\n",
        "    ax2.tick_params(axis='y', rotation=0, labelcolor='y' )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb_XPN3A4Abj"
      },
      "outputs": [],
      "source": [
        "plot_loss_and_acc(mathbert_loss_vals, mathbert_eval_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNR3xG4yDKVx"
      },
      "source": [
        "### Question 3: Freezing the pretrained weights [0.5pts]\n",
        "\n",
        "Now, lets try training the model again, except this time we will _not_ fine-tune BERTs weights (we sometimes say these weights are \"frozen\"). To do this, we will only compute gradients for the classifiers parameters.\n",
        "\n",
        "> We can do this in pytorch by setting the `requires_grad` attribute to `False` for all parameters beside the classifiers.\n",
        "\n",
        "Run the following cells to instantiate the model and train only the classifier. Then answer the follow-up questions in the assignment handout.\n",
        "\n",
        "> __Note__: This may print a warning: _\"Some weights of the model checkpoint at...\"_ which you can ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sPmVD0P3CyS"
      },
      "outputs": [],
      "source": [
        "mathbert_frozen = BertForSentenceClassification.from_pretrained(\n",
        "    \"tbs17/MathBERT\",  # the name of the pretrained model\n",
        "    num_labels=3,      # the number of classes in our downstream task\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-Y078xU3OGy"
      },
      "outputs": [],
      "source": [
        "for name, param in mathbert_frozen.named_parameters():\n",
        "\t# Only compute gradients for parameters of our\n",
        "\t# newly added classifier. BERT will not be trained.\n",
        "\tif 'classifier' not in name:\n",
        "\t\tparam.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__uaGXF13jXY"
      },
      "outputs": [],
      "source": [
        "# About 1 second per epoch on GPU\n",
        "mathbert_frozen_loss_vals, mathbert_frozen_eval_accs = train_model(\n",
        "    model=mathbert_frozen,\n",
        "    epochs=3, \n",
        "    train_dataloader=bert_train_dataloader,\n",
        "    validation_dataloader=bert_validation_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPqvEsUkLfCe"
      },
      "outputs": [],
      "source": [
        "plot_loss_and_acc(mathbert_frozen_loss_vals, mathbert_frozen_eval_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmbhFHISFXAo"
      },
      "source": [
        "### Question 4: Effect of pretraining data [0.5pts]\n",
        "\n",
        "Now, let's try fine-tuning the model again, except this time we will use [BERTweets](https://arxiv.org/abs/2005.10200) pretrained weights. BERTweets uses the same architecture as BERT (and MathBERT), but has been pretrained on 100s of millions of _tweets_.\n",
        "\n",
        "Run the following cells to instantiate our model with BERTweets pretrained weights and fine-tune it. Then answer the follow-up questions in the assignment handout.\n",
        "\n",
        "> __Note__: This may print a warning: _\"You are using a model of type...\"_ which you can ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yis-plPDEmlQ"
      },
      "outputs": [],
      "source": [
        "bertweet = BertForSentenceClassification.from_pretrained(\n",
        "    \"vinai/bertweet-base\",  # the name of the pretrained model\n",
        "    num_labels=3,           # the number of classes in our downstream task\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTweets has its own tokenizer, so we have to repeat the data loading process"
      ],
      "metadata": {
        "id": "EkOZiBbi1igb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bertweet_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', do_lower_case=True)\n",
        "\n",
        "tokenized_inputs = bertweet_tokenizer(\n",
        "    inputs.tolist(),\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    max_length=MAX_LEN,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "\n",
        "bert_train_dataloader, bert_validation_dataloader = train_valid_split(\n",
        "    input_ids=tokenized_inputs['input_ids'],\n",
        "    attention_masks=tokenized_inputs['attention_mask'],\n",
        "    labels=labels,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "kc1KjBbZa7GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCLobx6UFFe7"
      },
      "outputs": [],
      "source": [
        "# About 2-3 seconds per epoch on GPU\n",
        "bertweet_loss_vals, bertweet__eval_accs = train_model(\n",
        "    model=bertweet,\n",
        "    epochs=3, \n",
        "    train_dataloader=bert_train_dataloader,\n",
        "    validation_dataloader=bert_validation_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_and_acc(bertweet_loss_vals, bertweet__eval_accs)"
      ],
      "metadata": {
        "id": "4llKiw---Mil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAFzVhML8mru"
      },
      "source": [
        "### Question 5: Inspect models predictions [0pts]\n",
        "\n",
        "In the following cell, we have provided a function that allows you to inspect the models predictions. Given an input, e.g. `\"three minus two minus two\"`, it will return a trained models prediction i.e. `\"negative\"`, `\"zero\"`, or `\"positive\"`.\n",
        "\n",
        "Compare the performance of `mathbert`, `mathbert_frozen` and `bertweet`. Try a few unseen examples of arithmetic questions using all models. Can you find examples where one model clearly outperforms the others? Can you find examples where all models perform poorly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfHnZz-p-rTd"
      },
      "outputs": [],
      "source": [
        "def what_is(input, model, tokenizer):\n",
        "    # Use GPU, if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Get map of human readable outputs\n",
        "    index_to_sentiment_map = {0: \"negative\", 1: \"zero\", 2: \"positive\"}\n",
        "    \n",
        "    tokenized_inputs = tokenizer(\n",
        "        input,                    # Input text\n",
        "        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "        padding='max_length',     # pad to a length specified by the max_length\n",
        "        max_length=MAX_LEN,       # truncate all sentences longer than max_length\n",
        "        return_tensors='pt',      # return everything we need as PyTorch tensors\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized_inputs['input_ids'].to(device)\n",
        "    attention_masks = tokenized_inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        print(index_to_sentiment_map[np.argmax(logits, axis=1)[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCOApgNFK0q3"
      },
      "outputs": [],
      "source": [
        "what_is(\"three minus five\", model=mathbert, tokenizer=bert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CzHCK4qAzo_"
      },
      "outputs": [],
      "source": [
        "what_is(\"three minus five\", model=mathbert_frozen, tokenizer=bert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbr2Rm9pAuP7"
      },
      "outputs": [],
      "source": [
        "what_is(\"three minus five\", model=bertweet, tokenizer=bert_tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_Y6A-bYtNNnd",
        "3KSUpX1-P1uv",
        "l47jS3ifQJz3",
        "oyNFphizgCiM",
        "4RenfgCJg4Fi",
        "pkWOVuJohI3q",
        "PrvaiOR2hAEH",
        "wCMCg2B-xHZU",
        "Vm_NPA-o0m3v",
        "jkp-UJdO0xbF",
        "YmeBvaXY2Mcm",
        "wNR3xG4yDKVx",
        "RmbhFHISFXAo",
        "RAFzVhML8mru"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}