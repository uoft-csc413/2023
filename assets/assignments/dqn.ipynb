{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3szwJAdaoQa"
   },
   "source": [
    "## Enable rendering OpenAI Gym environments from CoLab\n",
    "\n",
    "In this assignemnt, We will use [OpenAI Gym](https://gym.openai.com/) for rendering game envionment for our agent to play and learn. It is possible and important to visualize the game your agent is playing, even on Colab. This section imports the necessary package and functions needed to generate a video in Colab. The video processing steps credit to [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9fFfA-gb8oC",
    "outputId": "79bb68ef-cdce-4b29-80e5-3fec571e3035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (60.10.0)\n"
     ]
    }
   ],
   "source": [
    "# You will need to run this block twice to make it effective\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install box2d-py > /dev/null 2>&1\n",
    "!pip install gym[Box_2D] > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NE1QQOVX27dp"
   },
   "outputs": [],
   "source": [
    "!pip install gym==0.15.3 pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfTPg6uZckCm"
   },
   "source": [
    "Import openAI gym and define the functions used to show the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLl9cs6ncAf0"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHkrflTWakKd"
   },
   "source": [
    "Import other packages:\n",
    "\n",
    "We will use Pytorch for building and learning our DQN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KvZYSl6RrzD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrLGCk-3ditk"
   },
   "source": [
    "## Run the game with random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMcGALcWeWfh"
   },
   "outputs": [],
   "source": [
    "from torch import randint\n",
    "from time import sleep\n",
    "\n",
    "env = wrap_env(gym.make('CartPole-v1'))\n",
    "reward_arr = []\n",
    "episode_count = 20\n",
    "for i in tqdm(range(episode_count)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    env.render()\n",
    "    while not done:\n",
    "        A = randint(0, env.action_space.n, (1,))\n",
    "        obs, reward, done, info = env.step(A.item())\n",
    "        rew += reward\n",
    "        sleep(0.01)\n",
    "    reward_arr.append(rew)\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ope0zHAjfXQh"
   },
   "source": [
    "The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center. The video is short (< 1s) because the pole loses balance immediately. \n",
    "\n",
    "You can see that a random agent is having trouble balancing the CartPole, just like you. However, a difficult game for human may be very simple to a computer. Let's see how we can use DQN to train a agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjP_2jn3SFgv"
   },
   "source": [
    "## Experience Replay\n",
    "\n",
    "The technique of experience replay was first proposed in to resolve temporal correlation in the input data by mixing recent experiences as well past experiences, essentially forcing the input to become independent and identically distributed (i.i.d.). It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoxW1Jlnk8mS"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "      def __init__(self, length):\n",
    "        self.experience_replay = deque(maxlen=length)\n",
    "\n",
    "      def collect(self, experience):\n",
    "        self.experience_replay.append(experience)\n",
    "        return\n",
    "\n",
    "      def sample_from_experience(self, sample_size):\n",
    "        if len(self.experience_replay) < sample_size:\n",
    "            sample_size = len(self.experience_replay)\n",
    "        sample = random.sample(self.experience_replay, sample_size)\n",
    "        state = torch.tensor([exp[0] for exp in sample]).float()\n",
    "        action = torch.tensor([exp[1] for exp in sample]).float()\n",
    "        reward = torch.tensor([exp[2] for exp in sample]).float()\n",
    "        next_state = torch.tensor([exp[3] for exp in sample]).float()\n",
    "        return state, action, reward, next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgXwmV2im4Sx"
   },
   "source": [
    "## Build our DQN Network\n",
    "\n",
    "We will use a simple multi-layer neural network to learn the optimal actions. We will use Adam Optimizor and MSE loss for training. **Notice that the loss function and gamma is given to you in the class attribute.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wmd1pfuRm7MQ"
   },
   "outputs": [],
   "source": [
    "class DQN_Network:\n",
    "\n",
    "    def __init__(self, layer_size_list, lr, seed=1423):\n",
    "        torch.manual_seed(seed)\n",
    "        self.policy_net = self.create_network(layer_size_list)\n",
    "        self.target_net = copy.deepcopy(self.policy_net)\n",
    "  \n",
    "        self.loss_fn = torch.nn.MSELoss() # the loss function\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "\n",
    "        self.step = 0\n",
    "        self.gamma = torch.tensor(0.95).float()\n",
    "        return\n",
    "\n",
    "    def create_network(self, layer_size_list):\n",
    "        assert len(layer_size_list) > 1\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(layer_size_list) - 1):\n",
    "            linear = nn.Linear(layer_size_list[i], layer_size_list[i + 1])\n",
    "\n",
    "            if i < len(layer_size_list) - 2:\n",
    "              activation = nn.Tanh()\n",
    "            else:\n",
    "              activation = nn.Identity()\n",
    "\n",
    "            layers += (linear, activation)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def load_pretrained_model(self, model_path):\n",
    "        self.policy_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def save_trained_model(self, model_path=\"cartpole-dqn.pth\"):\n",
    "        torch.save(self.policy_net.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKVV16YctASm"
   },
   "source": [
    "## **[Your task]**: complete the function that chooses the next action\n",
    "\n",
    "Choose next action based on **$\\epsilon$-greedy**:\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{a_{t+1}} = \\begin{cases}\n",
    "     \\text{argmax}_{a}Q(a, s)  & \\text{with probability }: 1 - \\epsilon, \\text{exploitation}\\\\\n",
    "     \\text{Uniform}\\{a_{1},...,a_{n}\\} & \\text{with probability}:   \\epsilon, \\text{exploration} \\\\\n",
    "   \\end{cases}\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE0gVweYs8xW"
   },
   "outputs": [],
   "source": [
    "def get_action(model, state, action_space_len, epsilon):\n",
    "    # We do not require gradient at this point, because this function will be used either\n",
    "    # during experience collection or during inference\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Qp = model.policy_net(torch.from_numpy(state).float())\n",
    "\n",
    "    ## TODO: select and return action based on epsilon-greedy\n",
    "  \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF9a5-IbazjQ"
   },
   "source": [
    "### **[Your task]**: complete the function that train the network for one step\n",
    "\n",
    "Here, you can find an ``train`` function that performs a\n",
    "single step of the optimization. \n",
    "\n",
    "For our training update rule, the loss you are trying to minimize is:\n",
    "\n",
    "\\begin{align}\\text{loss} = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtx4FAiab0Hp"
   },
   "outputs": [],
   "source": [
    "def train(model, batch_size):\n",
    "    state, action, reward, next_state = memory.sample_from_experience(sample_size=batch_size)\n",
    "\n",
    "    # TODO: predict expected return of current state using main network\n",
    "\n",
    "    # TODO: get target return using target network\n",
    "\n",
    "\n",
    "    # TODO: compute the loss\n",
    "    loss = \n",
    "    model.optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    model.optimizer.step()\n",
    "\n",
    "    model.step += 1\n",
    "    if model.step % 5 == 0:\n",
    "        model.target_net.load_state_dict(model.policy_net.state_dict())\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uskoe87Uz-Jg"
   },
   "source": [
    "### **[Your task]**: Finish the training loop\n",
    "\n",
    "In this part, you can play around with ```exp_replay_size```, ```episode```, ```epsilon``` and the \"episodo decay\" logic to train your model. **If you have done correctly, you will observe that the training time for the latter episodes is longer than the early episodes. This is because your agent is getting better and better at playing the game and thus each episode takes longer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NfNnyD6SPpN"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "env = gym.make('CartPole-v0')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "agent = DQN_Network(layer_size_list=[input_dim, 64, output_dim], lr=1e-3)\n",
    "\n",
    "# Main training loop\n",
    "losses_list, reward_list, episode_len_list, epsilon_list = [], [], [], []\n",
    "\n",
    "# TODO: try different values, it normally takes more than 6k episodes to train\n",
    "exp_replay_size = \n",
    "memory = ExperienceReplay(exp_replay_size)\n",
    "episodes = \n",
    "epsilon = 1 # episilon start from 1 and decay gradually. \n",
    "\n",
    "# initiliaze experiance replay\n",
    "index = 0\n",
    "for i in range(exp_replay_size):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        A = get_action(agent, obs, env.action_space.n, epsilon=1)\n",
    "        obs_next, reward, done, _ = env.step(A.item())\n",
    "        memory.collect([obs, A.item(), reward, obs_next])\n",
    "        obs = obs_next\n",
    "        index += 1\n",
    "        if index > exp_replay_size:\n",
    "            break\n",
    "\n",
    "index = 128\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs, done, losses, ep_len, rew = env.reset(), False, 0, 0, 0\n",
    "    while not done:\n",
    "        ep_len += 1\n",
    "        A = get_action(agent, obs, env.action_space.n, epsilon)\n",
    "        obs_next, reward, done, _ = env.step(A.item())\n",
    "        memory.collect([obs, A.item(), reward, obs_next])\n",
    "\n",
    "        obs = obs_next\n",
    "        rew += reward\n",
    "        index += 1\n",
    "\n",
    "        if index > 128:\n",
    "            index = 0\n",
    "            for j in range(4):\n",
    "                loss = train(agent, batch_size=16)\n",
    "                losses += loss\n",
    "    \n",
    "    # TODO: add epsilon decay rule here! \n",
    "\n",
    "\n",
    "    losses_list.append(losses / ep_len), reward_list.append(rew)\n",
    "    episode_len_list.append(ep_len), epsilon_list.append(epsilon)\n",
    "\n",
    "print(\"Saving trained model\")\n",
    "agent.save_trained_model(\"cartpole-dqn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCow7jNXf5YT"
   },
   "source": [
    "## Last Step: evaluate your trained model! **Make sure to include your visualizations (plot+video) in the notebook for your submission!**\n",
    "\n",
    "First we can plot the reward vs. episode. **If you have done correctly, you should see the reward can stabilize at 200 in later episodes**\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Hy_FP7yeXA4"
   },
   "outputs": [],
   "source": [
    "def plot_reward(r):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Result')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(r)\n",
    "\n",
    "plot_reward(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EixOpGakoyi4"
   },
   "source": [
    "Next let check out how well your agent plays the game. **If you have done correctly, you should see a relatively longer video (> 3~4s) with a self-balancing pole.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMEivKldTGVG"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v1'))\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "model_validate = DQN_Network(layer_size_list=[input_dim, 64, output_dim], lr=1e-3)\n",
    "model_validate.load_pretrained_model(\"cartpole-dqn.pth\")\n",
    "\n",
    "reward_arr = []\n",
    "for i in tqdm(range(200)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    env.render()\n",
    "    while not done:\n",
    "        A = get_action(model_validate, obs, env.action_space.n, epsilon=0)\n",
    "        obs, reward, done, info = env.step(A.item())\n",
    "        rew += reward\n",
    "        # sleep(0.01)\n",
    "\n",
    "    reward_arr.append(rew)\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTAa6RAWTKZJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "a4_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
